{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression II:\n",
    " - Random Feature Models\n",
    " - Ridge Regression\n",
    " - Cross-Validation\n",
    " - XGBoost\n",
    " \n",
    " ---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do two experiments.\n",
    "\n",
    "- Experiment A: Try to predict Google's stock using a historical rolling window.\n",
    "\n",
    "- Experiment B: Try to predict Google's stock using Apple's stock.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "Play around with some hyperparameters and see how the experiment's results change.  E.g.\n",
    "\n",
    " - The feature space's dimension,\n",
    " - The test set's size..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set True for Experiment A and False for Experiment B!\n",
    "Experiment_A = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we predict the price itself, or the price movement?  I.e. $X_t$ vs. the *logarithmic returns* $\\log(\\frac{X_{t+1}}{X_t})$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Returns = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Let's start with the basics and some artificial data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize/Import some basic packages\n",
    "\n",
    "*Make sure to comment your code so you can remember what it does in 2+ days :P*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Let's ignore warnings (they're annoying and we know what's up)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Some HTML commands for the notebook\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our real data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This package pulls, data from finance\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll be randomizing things; we want our experiments to be reproducible...\n",
    "So let's set a seed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(2022) # Numpy uses a different (random) seed :/\n",
    "random.seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's gather our hyperparameters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of testin points\n",
    "N_test = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortly, we'll want to pick a large feature space dimension \"$F$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for Random Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Space's Dimension\n",
    "dim_feature = 10**4\n",
    "\n",
    "# Radius for sampling random weights\n",
    "radius_weights = 0.25 # <- Should be positive\n",
    "# Radius for sampling random biases\n",
    "radius_bias = 1.1 # <- Should be bigger than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and Training our Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linReg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As last week: \n",
    "We will pull, google stock's daily closing prices from Yahoo finance as covariates.\n",
    "The targets will be Google's next day price :)\n",
    "\n",
    "We'll train on a daily 5 year period!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull\n",
    "data_raw = yf.download('GOOG','2008-01-01','2020-12-31')\n",
    "# check\n",
    "print('Tail')\n",
    "print(data_raw.tail())\n",
    "print('Head')\n",
    "print(data_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Keep only the Closing Prices as covariates\n",
    "# 2) Convert to Dataframe type (Manipulation of data is easier for this object class; as opposed to numpy arrays)\n",
    "data_close = pd.DataFrame(data_raw.Close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Seaborn (<- this makes plots pretty :3)\n",
    "sns.set()\n",
    "# Set plot Size to display\n",
    "plt.figure(figsize=(12, 12), dpi=80)\n",
    "\n",
    "# Plot!\n",
    "plt.plot(data_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Experiment_A == True:\n",
    "    # The covariates \n",
    "    # The inputs will be the rolling average of the last 3 days\n",
    "    data = data_close\n",
    "    data['Rolling_Mean_Historical'] = data_close.shift(1).rolling(window=3).mean() \n",
    "    data = data.dropna()\n",
    "\n",
    "    # Lets divide things up into targets and covariates\n",
    "    Y = pd.DataFrame(data['Close'])\n",
    "    X = pd.DataFrame(data['Rolling_Mean_Historical'])\n",
    "else:\n",
    "    # Pull\n",
    "    Y_data_raw = yf.download('GOOG','2018-01-03','2020-12-30')\n",
    "    # check\n",
    "    print('Tail')\n",
    "    print(Y_data_raw.tail())\n",
    "    print('Head')\n",
    "    print(Y_data_raw.head())\n",
    "\n",
    "    # Pull Covariates\n",
    "    X_data_raw = yf.download('AAPL','2018-01-02','2020-12-29')\n",
    "    # check\n",
    "    print('Tail')\n",
    "    print(X_data_raw.tail())\n",
    "    print('Head')\n",
    "    print(X_data_raw.head())\n",
    "        \n",
    "        \n",
    "    #NB: This is the same as in lecture 0\n",
    "    \n",
    "    # 1) Keep only the Closing Prices as covariates\n",
    "    # 2) Convert to Dataframe type (Manipulation of data is easier for this object class; as opposed to numpy arrays)\n",
    "    Y = pd.DataFrame(Y_data_raw.Close)\n",
    "    X = pd.DataFrame(X_data_raw.Close)\n",
    "\n",
    "    # Just double-check arrays are of the same size (note Jan. 1rst is a holiday)\n",
    "    print(X.shape)\n",
    "    print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the log returns for X and for Y; NB we drop the first row since it is NAN by definition of the next command\n",
    "if Log_Returns == True:\n",
    "    # Compute the log return for X\n",
    "    X = np.log1p(X.pct_change()).dropna()\n",
    "    # Compute the log return for Y\n",
    "    Y = np.log1p(Y.pct_change()).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the last 20 days \"invisible\" and use them as our testing set (we'll pretend those are the too be predicted prices!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify number of training datapoints\n",
    "N_train = Y.shape[0]-N_test\n",
    "\n",
    "# Build Train\n",
    "Y_train = Y[:N_train]\n",
    "X_train = X[:N_train]\n",
    "\n",
    "# Build Test\n",
    "Y_test = Y[N_train:]\n",
    "X_test = X[N_train:]\n",
    "\n",
    "\n",
    "# Visualize Dataframe Dimensions (make sure things are running reasonably!)\n",
    "print('Check Train')\n",
    "print(X_train.shape)\n",
    "print(X_train.head())\n",
    "print('Check Test')\n",
    "print(X_test.shape)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## We'll Initialize our Benchmark model\n",
    "\n",
    "This simple/basic/vanilla model will serve as our target to beat; if we don't then we're doing something wrong :S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and Training our Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linReg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we train our model, with the \"training\" (a.k.a. in-sample) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linReg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll predict the test data given our input train data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prediction\n",
    "Yhat_test_linReg = model_linReg.predict(X_test)\n",
    "# Convert to \"vector shape\"\n",
    "Yhat_test_linReg = Yhat_test_linReg.reshape([-1,])\n",
    "# Visualize to make sure things look okay!\n",
    "Yhat_test_linReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll record our training performance while we're at it also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prediction\n",
    "Yhat_train_linReg = model_linReg.predict(X_train)\n",
    "# Convert to \"vector shape\"\n",
    "Yhat_train_linReg = Yhat_train_linReg.reshape([-1,])\n",
    "# Visualize to make sure things look okay!\n",
    "Yhat_train_linReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check how we did!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dataframe\n",
    "\n",
    "*This is not the most efficient coding; but the point is rather to show you how to name and join columns to dataframes :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a dataframe with the target and our predictions as columns\n",
    "time_series_comparison = pd.DataFrame(Y_test)\n",
    "time_series_comparison.columns = ['Targets_Test'] # <- Rename columns\n",
    "\n",
    "# Append Predictions to Dataframe\n",
    "time_series_comparison['Predictions_Test'] = Yhat_test_linReg\n",
    "\n",
    "# Check nothing went wrong (Safefy First!)\n",
    "time_series_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Like Last week We Visualize how things are panning our so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Seaborn (<- this makes plots pretty :3)\n",
    "sns.set()\n",
    "# Set plot Size to display\n",
    "plt.figure(figsize=(12, 12), dpi=80)\n",
    "\n",
    "# Plot!\n",
    "plt.plot(time_series_comparison)\n",
    "\n",
    "# Now we'll need a legend also\n",
    "plt.legend(time_series_comparison.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to try our Random Feature model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's initialize our hyperparameters + latent parameters\n",
    "These will be used in generating our (random) feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_data = X_train.shape[1] # <- Number of covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well get our first \"helper function\".  These are intended to be used often (like lemata) to help with small individual tasks, which we frequently perform.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform Sampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_ball(num_points = dim_feature, dimension=dim_data, radius=1):\n",
    "    from numpy import random, linalg\n",
    "    # First generate random directions by normalizing the length of a\n",
    "    # vector of random-normal values (these distribute evenly on ball).\n",
    "    random_directions = random.normal(size=(dimension,num_points))\n",
    "    random_directions /= linalg.norm(random_directions, axis=0)\n",
    "    # Second generate a random radius with probability proportional to\n",
    "    # the surface area of a ball with a given radius.\n",
    "    random_radii = random.random(num_points) ** (1/dimension)\n",
    "    # Return the list of random (direction & length) points.\n",
    "    return radius * (random_directions * random_radii).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next let's write a little function to generate random features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have set the correct default parameters for the theorem...let's just run it with an empty argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random parameters determining our feature map\n",
    "rand_weights = random_ball(radius=radius_weights)\n",
    "rand_biases = random_ball(dimension = 1,radius=radius_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: It's crucial to initialize the random matrix and random vectors externally to the random feature map.  Otherwise, everytime you run the code you'll be multiplying by a different matrix + adding a different vector.\n",
    "\n",
    "*I.e.: You'll turn your features into pure noise :0*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_feature_map(x_input):\n",
    "    x_int = np.array(x_input).transpose()\n",
    "    #Apply Random Weights\n",
    "    x_int = rand_weights.dot(x_int).transpose()\n",
    "    #Apply Random Bias\n",
    "    x_int = x_int + rand_biases.transpose()\n",
    "    #Apply ReLU activation function\n",
    "    x_int = np.maximum(x_int,0)\n",
    "    #Return Random Features\n",
    "    return x_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply our random feature map to each of our data points in X\n",
    "\n",
    "This gives us our random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply to each data\n",
    "## first to the training dataset\n",
    "X_Rand_Features_train = rand_feature_map(X_train)\n",
    "X_Rand_Features_train = pd.DataFrame(X_Rand_Features_train)\n",
    "## then to the testing dataset\n",
    "X_Rand_Features_test = rand_feature_map(X_test)\n",
    "X_Rand_Features_test = pd.DataFrame(X_Rand_Features_test)\n",
    "\n",
    "# Vislualize things to make sure all is working\n",
    "print('Train Shape')\n",
    "print(X_Rand_Features_train.shape)\n",
    "print(X_Rand_Features_train.head())\n",
    "print('Test Shape')\n",
    "print(X_Rand_Features_test.shape)\n",
    "print('Test Features')\n",
    "X_Rand_Features_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also include the original (untransformed features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Rand_Features_train['Original_Features'] = np.array(X_train)\n",
    "X_Rand_Features_test['Original_Features'] = np.array(X_test)\n",
    "\n",
    "# Let's check all is working well\n",
    "print(X_Rand_Features_train.head())\n",
    "X_Rand_Features_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before...\n",
    "\n",
    "#### Building and Training our Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linReg_randfeatures = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we train our model, with the \"training\" (a.k.a. in-sample) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_linReg_randfeatures.fit(X_Rand_Features_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It remains to train our random neural network (or random feature model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "model_linReg_randfeatures = LinearRegression()\n",
    "# Train Model\n",
    "# fit model\n",
    "model_linReg_randfeatures.fit(X_Rand_Features_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll predict the test data given our input train data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prediction\n",
    "Yhat_train_linReg_RF = model_linReg_randfeatures.predict(X_Rand_Features_train)\n",
    "Yhat_train_linReg_RF = Yhat_train_linReg_RF.reshape([-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll record our training performance while we're at it also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_test_linReg_RF = model_linReg_randfeatures.predict(X_Rand_Features_test)\n",
    "# Yhat_test_linReg = pd.DataFrame(Yhat_test_linReg)\n",
    "Yhat_test_linReg_RF = Yhat_test_linReg_RF.reshape([-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how our new random feature model measures up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Predictions to Dataframe\n",
    "time_series_comparison['Predictions_Test_RF'] = Yhat_test_linReg_RF\n",
    "\n",
    "# Check nothing went wrong (Safefy First!)\n",
    "Yhat_test_linReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Visualize How we did with our Random Features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Seaborn (<- this makes plots pretty :3)\n",
    "sns.set()\n",
    "# Set plot Size to display\n",
    "plt.figure(figsize=(8, 8), dpi=70)\n",
    "\n",
    "# Plot!\n",
    "plt.plot(time_series_comparison)\n",
    "\n",
    "# Now we'll need a legend also\n",
    "plt.legend(time_series_comparison.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better look, let's plot the time-series of squared prediction errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some statistics to see when and if we ever do better with the random feature?\n",
    "\n",
    "Typically, is:\n",
    "\n",
    "time_series_comparison_errors['Lin. Reg.'] > time_series_comparison_errors['Lin. Reg. + RF']?\n",
    "\n",
    "I.e. is this quantity positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timeseries of Squred prediction errors, for vanilla regression model!\n",
    "SSE_LinReg = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test'])**2\n",
    "\n",
    "# Get timeseries of Squred prediction errors, for random feature model!\n",
    "SSE_LinReg_RF = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test_RF'])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a pandas dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Predictions to Dataframe\n",
    "time_series_comparison_errors = pd.DataFrame({'Lin. Reg.':SSE_LinReg,'Lin. Reg. + RF':SSE_LinReg_RF})\n",
    "\n",
    "# Check nothing went wrong (Safefy First!)\n",
    "time_series_comparison_errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average gain by random features') \n",
    "np.mean(time_series_comparison_errors['Lin. Reg.']-time_series_comparison_errors['Lin. Reg. + RF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the variance in the two model's errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variance')\n",
    "np.var(time_series_comparison_errors['Lin. Reg.']-time_series_comparison_errors['Lin. Reg. + RF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often are the random features paying off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Linear Regression has a larger error')\n",
    "print(np.mean(time_series_comparison_errors['Lin. Reg.']>time_series_comparison_errors['Lin. Reg. + RF']))\n",
    "print('percent of the time!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our regressor, after having generated random features, with a stabler algorithm; namely the *ridge regression* method we have seen in class.\n",
    "\n",
    "We will **tune** the hyperparameters using [($k$-fold) cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) using what's built into the [sklearn package](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages for grid searching (hyperparameter tuning)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and Training our Regression Model\n",
    "\n",
    "We first initialize our regression model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Ridge_randfeatures = Ridge()\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=20, random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our grid of hyperparameters defining the ridge regression.  These will be used to cross-validate our model's hyperparameter *(namely the $\\lambda\\ge 0$ in the course notes)*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the grid\n",
    "Ridge_hyperparameters = dict()\n",
    "Ridge_hyperparameters['alpha'] = np.arange(0, 1, 0.01)\n",
    "\n",
    "# Then we state how we search (namely randomized for speed) and using what criterion \n",
    "# By which we judge a model's quality on the validation set\n",
    "search = RandomizedSearchCV(model_Ridge_randfeatures, \n",
    "                            Ridge_hyperparameters, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            cv=4, \n",
    "                            n_jobs=-1,\n",
    "                            n_iter = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we perform the $4$-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the randomized grid search\n",
    "results = search.fit(X_Rand_Features_train, Y_train)\n",
    "# Save Best Ridge Parameter\n",
    "best_ridge_parameter = results.best_params_['alpha']\n",
    "# Summarize the result (we use mean absolute error)\n",
    "print('The best MSE is: %.3f' % results.best_score_)\n",
    "print('The best Ridge-regression hyperparameter is: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define and train our ridge model with the best parameter identified by cross-validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_Ridge_randfeatures = Ridge(alpha = best_ridge_parameter)\n",
    "model_Ridge_randfeatures.fit(X_Rand_Features_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll predict the test data given our input train data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prediction\n",
    "Yhat_train_ridgeReg_RF = model_Ridge_randfeatures.predict(X_Rand_Features_train)\n",
    "Yhat_train_ridgeReg_RF = Yhat_train_ridgeReg_RF.reshape([-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll record our training performance while we're at it also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_test_ridgeReg_RF = model_Ridge_randfeatures.predict(X_Rand_Features_test)\n",
    "# Yhat_test_linReg = pd.DataFrame(Yhat_test_linReg)\n",
    "Yhat_test_ridgeReg_RF = Yhat_test_ridgeReg_RF.reshape([-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how our new random feature model trained with ridge regression works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Predictions to Dataframe\n",
    "time_series_comparison['Predictions_Test_RF_Ridge'] = Yhat_test_ridgeReg_RF\n",
    "\n",
    "# Check nothing went wrong (Safefy First!)\n",
    "Yhat_test_ridgeReg_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Visualize How we did with our Random Features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Seaborn (<- this makes plots pretty :3)\n",
    "sns.set()\n",
    "# Set plot Size to display\n",
    "plt.figure(figsize=(12, 12), dpi=80)\n",
    "\n",
    "# Plot!\n",
    "plt.plot(time_series_comparison)\n",
    "\n",
    "# Now we'll need a legend also\n",
    "plt.legend(time_series_comparison.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timeseries of Squred prediction errors, for vanilla regression model!\n",
    "SSE_LinReg = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test'])**2\n",
    "\n",
    "# Get timeseries of Squred prediction errors, for random feature model!\n",
    "SSE_LinReg_RF = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test_RF'])**2\n",
    "\n",
    "# Get timeseries of Squred prediction errors, for random feature model trained with ridge regression!\n",
    "SSE_LinReg_ridge = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test_RF_Ridge'])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a pandas dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Predictions to Dataframe\n",
    "time_series_comparison_errors = pd.DataFrame({'Lin. Reg.':SSE_LinReg,'Lin. Reg. + RF':SSE_LinReg_RF, 'Ridge Reg. + RF': SSE_LinReg_ridge})\n",
    "\n",
    "# Check nothing went wrong (Safefy First!)\n",
    "time_series_comparison_errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Activate Seaborn (<- this makes plots pretty :3)\n",
    "sns.set()\n",
    "# Set plot Size to display\n",
    "plt.figure(figsize=(6,6), dpi=80)\n",
    "\n",
    "# Plot!\n",
    "plt.plot(time_series_comparison_errors)\n",
    "\n",
    "# Now we'll need a legend also\n",
    "plt.legend(time_series_comparison_errors.columns);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Model Performance Statistically\n",
    "\n",
    "Well get a quantitative understanding of how our models performed statistically, by looking at some descriptive statistics such as the \"mean test set's error\" and the \"variance of our test set's errors\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_comparison_errors.mean()[np.argsort(time_series_comparison_errors.mean())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_series_comparison_errors.var()[np.argsort(time_series_comparison_errors.var())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our Random Feature Model with the Huber Loss Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the package of regression with the Huber loss function\n",
    "from sklearn.linear_model import HuberRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and Training our Regression Model\n",
    "\n",
    "We first initialize our regression model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_HuberRegression_randfeatures = HuberRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define and train our ridge model with the best parameter identified by cross-validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_HuberRegression_randfeatures.fit(X_Rand_Features_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll predict the test data given our input train data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prediction\n",
    "Yhat_train_HuberReg_RF = model_HuberRegression_randfeatures.predict(X_Rand_Features_train)\n",
    "Yhat_train_HuberReg_RF = Yhat_train_HuberReg_RF.reshape([-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll record our training performance while we're at it also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_test_HubertReg_RF = model_HuberRegression_randfeatures.predict(X_Rand_Features_test)\n",
    "# Yhat_test_linReg = pd.DataFrame(Yhat_test_linReg)\n",
    "Yhat_test_HubertReg_RF = Yhat_test_HubertReg_RF.reshape([-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how our new random feature model trained with ridge regression works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Predictions to Dataframe\n",
    "time_series_comparison['Predictions_Test_RF_Huber'] = Yhat_test_HubertReg_RF\n",
    "\n",
    "# Check nothing went wrong (Safefy First!)\n",
    "Yhat_test_HubertReg_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Visualize How we did with our Random Features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Seaborn (<- this makes plots pretty :3)\n",
    "sns.set()\n",
    "# Set plot Size to display\n",
    "plt.figure(figsize=(12, 12), dpi=80)\n",
    "\n",
    "# Plot!\n",
    "plt.plot(time_series_comparison)\n",
    "\n",
    "# Now we'll need a legend also\n",
    "plt.legend(time_series_comparison.columns);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timeseries of Squred prediction errors, for vanilla regression model!\n",
    "SSE_LinReg = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test'])**2\n",
    "\n",
    "# Get timeseries of Squred prediction errors, for random feature model!\n",
    "SSE_LinReg_RF = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test_RF'])**2\n",
    "\n",
    "# Get timeseries of Squred prediction errors, for random feature model trained with ridge regression!\n",
    "SSE_LinReg_ridge = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test_RF_Ridge'])**2\n",
    "\n",
    "# Get timeseries of Squred prediction errors, for random feature model trained with Huber loss!\n",
    "SSE_LinReg_Huber = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test_RF_Huber'])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a pandas dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Predictions to Dataframe\n",
    "time_series_comparison_errors = pd.DataFrame({'Lin. Reg.':SSE_LinReg,'Lin. Reg. + RF':SSE_LinReg_RF, 'Ridge Reg. + RF': SSE_LinReg_ridge, 'Huber Loss Reg.+ RF':SSE_LinReg_Huber})\n",
    "\n",
    "# Check nothing went wrong (Safefy First!)\n",
    "time_series_comparison_errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Activate Seaborn (<- this makes plots pretty :3)\n",
    "sns.set()\n",
    "# Set plot Size to display\n",
    "plt.figure(figsize=(6,6), dpi=80)\n",
    "\n",
    "# Plot!\n",
    "plt.plot(time_series_comparison_errors)\n",
    "\n",
    "# Now we'll need a legend also\n",
    "plt.legend(time_series_comparison_errors.columns);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Model Performance Statistically\n",
    "\n",
    "Well get a quantitative understanding of how our models performed statistically, by looking at some descriptive statistics such as the \"mean test set's error\" and the \"variance of our test set's errors\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_comparison_errors.mean()[np.argsort(time_series_comparison_errors.mean())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_series_comparison_errors.var()[np.argsort(time_series_comparison_errors.var())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Thoughts:\n",
    "\n",
    "Linear regression, with its many variants does an alright job at making predictions.  However, all in all, its a bit underwhealming and overly simplistic (imo).  \n",
    "\n",
    "---\n",
    "\n",
    "# Playing with Our First *Serious* ML Algorithm\n",
    "\n",
    "However, we're now ready to take things to the next level; and really consider a simple but so powerful machine learning algorithm called *gradient boosting*.  Moreover, this regression algorithm is implemented super efficiently in the [*XGBoost* Python Package](https://xgboost.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of your best friends when implementing ML algorithms will be [*XGBoost*](https://en.wikipedia.org/wiki/XGBoost).  This intuitive, lightning-fast, yet extremely powerful learning algorithm is very difficult to beat in practice and should (imo) as your go-to second benchmark (or even final ML model). \n",
    "\n",
    "This time, our (much richer) hypothesis class consists of regression trees.  These are a specific subclass of the set of **piecewise constant** functions (think simple functions like in Ito/stochastic integration theory which you've seen in your other course).  I.e. our hypothesis/models are of the form\n",
    "$$\n",
    "\\hat{f}(x) := \\sum_{n=1}^N\\, y_n\\, I_{[a_1^n,b_1^n]\\times \\dots \\times [a_d^n,b_d^n]}(x)\n",
    "$$\n",
    "plus some additional \"tree-structure\" amongst the little cubes $[a_1^n,b_1^n]\\times \\dots \\times [a_d^n,b_d^n]$ in $\\mathbb{R}^d$, \n",
    "*(for some vectors $y_n\\in \\mathbb{R}^D$, some real-numbers $a_i<b_i$, and some positive integer $N$)*.\n",
    "\n",
    "![Image of Yaktocat](https://www.nvidia.com/content/dam/en-zz/Solutions/glossary/data-science/xgboost/img-3.png)\n",
    "\n",
    "The tree structure is used to descide how we chop up our input space $\\mathcal{X}$ in $\\mathbb{R}^d$ into $N$ tiny little cubes: \n",
    " - $[a_1^1,b_1^1]\\times \\dots \\times [a_d^1,b_d^1]$,\n",
    " - ...\n",
    " - $[a_1^n,b_1^n]\\times \\dots \\times [a_d^N,b_d^N]$.\n",
    " \n",
    " They key point is how the algorithm chops up the space.  In brief, it first making a single prediction.  Then, it identifies the region in $\\mathbb{R}^d$ containing the most poorly predicted datapoints.  We then divid the input space into two parts; where the data was predicted well and where it was not.  We then perform another regression focusing on the points which were poorly predicted.  \n",
    " \n",
    "...\n",
    " \n",
    "This procedue is then iterated again and again, until we are satisfied with our result.  This proceduring is known in machine learning as **boosting**.  \n",
    "\n",
    "**Note:** We identify which points our model performs the worst on, by identifying the points whereon our loss-functions' *gradient* (i.e. its partial *derivatives*) are steepest!  This is where the term \"gradient\" arises in the algorithm's name: XGBoost or: (extreme) **gradient** boosting!\n",
    "\n",
    "**Note II:** The extreme in XGBoost refers to the randomization used in the algorithm *(see [extreme machine learning](https://en.wikipedia.org/wiki/Extreme_learning_machine) for further details)*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's get started by importing everyting; we'll work from xbgoost's [XGBRegressor implementation](https://xgboost.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a final xgboost model on the housing dataset and make a prediction\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "data = dataframe.values\n",
    "# split dataset into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# define model\n",
    "model = XGBRegressor()\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# define new data\n",
    "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "new_data = asarray([row])\n",
    "# make a prediction\n",
    "yhat = model.predict(new_data)\n",
    "# summarize prediction\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Building our XGBoost model using the classical features (just in X) as well as our random features (used in our random feature regressors thus far) and compare!\n",
    "\n",
    "We first initialize our regression model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGBoost = XGBRegressor(seed=2022)\n",
    "model_XGBoost_RF = XGBRegressor(seed=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train both models, on their respective training sets (i.e. with X and with our random features $\\phi(X)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model with standard features (just X)\n",
    "model_XGBoost.fit(X_train, Y_train)\n",
    "# Train model with random features (phi(X))\n",
    "model_XGBoost_RF.fit(X_Rand_Features_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how much improvement we can get if we actually optimize XGBoost's hyperparameters.\n",
    "\n",
    "As before, we use $4$-fold cross validation; which is built into sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XGBoost Model\n",
    "model_XGBoost_Optimized = XGBRegressor(seed=2022)\n",
    "\n",
    "# Let's define our hyperparameter grid \n",
    "XGBoost_Hyperparameters = {\n",
    "    'max_depth': range (3, 8, 1),\n",
    "    'max_leaves': range (1, 5, 1),\n",
    "    'n_estimators': range(200, 400, 20),\n",
    "    'learning_rate': np.linspace(0.1, 0.5,20)\n",
    "}\n",
    "\n",
    "# As before, we'll do a (randomized) grid search amongst the model's possible hyperparameter combinations\n",
    "XGBoost_grid_search = RandomizedSearchCV(model_XGBoost_Optimized,\n",
    "                                         XGBoost_Hyperparameters,\n",
    "                                         scoring = 'neg_mean_absolute_error', \n",
    "                                         cv=2, \n",
    "                                         n_jobs=-1,\n",
    "                                         n_iter = 5,\n",
    "                                         verbose = True)\n",
    "\n",
    "# We perform the grid search\n",
    "XGBoost_grid_search.fit(X_Rand_Features_train,Y_train)\n",
    "\n",
    "# We then update/extract our best XGBoost model's/parameters\n",
    "model_XGBoost_Optimized = XGBoost_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll predict the test data given our input train data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict model with standard features (just X)\n",
    "Yhat_train_XGBoost = model_XGBoost.predict(X_train)\n",
    "Yhat_train_XGBoost = Yhat_train_XGBoost.reshape([-1,])\n",
    "\n",
    "\n",
    "\n",
    "# Predict model with random features (phi(X))\n",
    "Yhat_train_XGBoost_RF = model_XGBoost_RF.predict(X_Rand_Features_train)\n",
    "Yhat_train_XGBoost_RF = Yhat_train_XGBoost_RF.reshape([-1,])\n",
    "\n",
    "\n",
    "# Predict model with standard features (phi(X)) but optimized hyperparameters\n",
    "Yhat_train_XGBoost_Optimized = model_XGBoost_Optimized.predict(X_Rand_Features_train)\n",
    "Yhat_train_XGBoost_Optimized = Yhat_train_XGBoost_Optimized.reshape([-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll record our training performance while we're at it also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict model with standard features (just X)\n",
    "Yhat_test_XGBoost_test = model_XGBoost.predict(X_test)\n",
    "Yhat_test_XGBoost_test = Yhat_test_XGBoost_test.reshape([-1,])\n",
    "\n",
    "\n",
    "\n",
    "# Predict model with random features (phi(X))\n",
    "Yhat_test_XGBoost_RF_test = model_XGBoost_RF.predict(X_Rand_Features_test)\n",
    "Yhat_test_XGBoost_RF_test = Yhat_test_XGBoost_RF_test.reshape([-1,])\n",
    "\n",
    "# Predict model with standard features (just X) but optimized hyperparameters\n",
    "Yhat_test_XGBoost_Optimized_test = model_XGBoost_Optimized.predict(X_Rand_Features_test)\n",
    "Yhat_test_XGBoost_Optimized_test = Yhat_test_XGBoost_Optimized_test.reshape([-1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Predictions to Dataframe\n",
    "time_series_comparison['Yhat_Test_XGBoost'] = Yhat_test_XGBoost_test\n",
    "time_series_comparison['Yhat_Test_XGBoost_RF'] = Yhat_test_XGBoost_RF_test\n",
    "time_series_comparison['Yhat_Test_XGBoost_Optim'] = Yhat_test_XGBoost_Optimized_test\n",
    "\n",
    "# Check nothing went wrong (Safefy First!)\n",
    "print(Yhat_test_XGBoost_test)\n",
    "print(Yhat_test_XGBoost_RF_test)\n",
    "print(Yhat_test_XGBoost_Optimized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Visualize How we did with our Random Features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Seaborn (<- this makes plots pretty :3)\n",
    "sns.set()\n",
    "# Set plot Size to display\n",
    "plt.figure(figsize=(12, 12), dpi=80)\n",
    "\n",
    "# Plot!\n",
    "plt.plot(time_series_comparison)\n",
    "\n",
    "# Now we'll need a legend also\n",
    "plt.legend(time_series_comparison.columns);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timeseries of Squred prediction errors, for vanilla regression model!\n",
    "SSE_LinReg = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test'])**2\n",
    "\n",
    "# Get timeseries of Squred prediction errors, for random feature model!\n",
    "SSE_LinReg_RF = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test_RF'])**2\n",
    "\n",
    "# Get timeseries of Squred prediction errors, for random feature model trained with ridge regression!\n",
    "SSE_LinReg_ridge = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test_RF_Ridge'])**2\n",
    "\n",
    "# Get timeseries of Squred prediction errors, for random feature model trained with Huber loss!\n",
    "SSE_LinReg_Huber = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Predictions_Test_RF_Huber'])**2\n",
    "\n",
    "\n",
    "\n",
    "# Get timeseries of Squred prediction errors, XGBoost model with no random features\n",
    "SSE_XGBoost = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Yhat_Test_XGBoost'])**2\n",
    "# Get timeseries of Squred prediction errors, XGBoost model with random features\n",
    "SSE_XGBoost_RF = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Yhat_Test_XGBoost_RF'])**2\n",
    "# Get timeseries of Squred prediction errors, XGBoost model with random features\n",
    "SSE_XGBoost_Optim = np.abs(time_series_comparison['Targets_Test'] - time_series_comparison['Yhat_Test_XGBoost_Optim'])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a pandas dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Predictions to Dataframe\n",
    "time_series_comparison_errors = pd.DataFrame({'Lin. Reg.':SSE_LinReg,\n",
    "                                              'Lin. Reg. + RF':SSE_LinReg_RF,\n",
    "                                              'Ridge Reg. + RF': SSE_LinReg_ridge,\n",
    "                                              'Huber Loss Reg.+ RF':SSE_LinReg_Huber,\n",
    "                                              'XGBoost':SSE_XGBoost,\n",
    "                                              'XGBoost + RF':SSE_XGBoost_RF,\n",
    "                                              'XGBoost (Optimized)':SSE_XGBoost_Optim})\n",
    "\n",
    "# Check nothing went wrong (Safefy First!)\n",
    "time_series_comparison_errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Activate Seaborn (<- this makes plots pretty :3)\n",
    "sns.set()\n",
    "# Set plot Size to display\n",
    "plt.figure(figsize=(6,6), dpi=80)\n",
    "\n",
    "# Plot!\n",
    "plt.plot(time_series_comparison_errors)\n",
    "\n",
    "# Now we'll need a legend also\n",
    "plt.legend(time_series_comparison_errors.columns);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Model Performance Statistically\n",
    "\n",
    "Well get a quantitative understanding of how our models performed statistically, by looking at some descriptive statistics such as the \"mean test set's error\" and the \"variance of our test set's errors\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_comparison_errors.mean()[np.argsort(time_series_comparison_errors.mean())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_series_comparison_errors.var()[np.argsort(time_series_comparison_errors.var())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's visualize how its test errors behave by visually examining the distribution of our best model's test erros.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which model performs best\n",
    "errors_best_model_test = time_series_comparison_errors.iloc[:,np.array(np.argsort(time_series_comparison_errors.mean())==1)]\n",
    "\n",
    "\n",
    "# Plot\n",
    "## Set plot Size to display\n",
    "plt.figure(figsize=(6,6), dpi=80)\n",
    "## Plot!\n",
    "plt.hist(errors_best_model_test, density=True, bins=30)  # density=False would make counts\n",
    "## Now we'll need a legend also\n",
    "plt.legend('Tets Error Distribution');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
